Some weights of GPT2LMHeadWithPCWModel were not initialized from the model checkpoint at gpt2 and are newly initialized because the shapes did not match:
- h.0.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.1.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.2.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.3.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.4.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.5.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.6.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.7.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.8.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.9.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.10.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
- h.11.attn.bias: found shape torch.Size([1, 1, 1024, 1024]) in the checkpoint and torch.Size([1, 1, 3072, 3072]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Found cached dataset ag_news (/home/b.nativi/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 582.91it/s]
loaded 120000 training samples & 7600 test samples
overriding default label mapping
['World -> world', 'Sports -> sports', 'Business -> business', 'Sci/Tech -> technology']
filtering test set:
filtered 73 from  dataset due to extreme length
longest remaining prompt according to tokenizer: 124
filtering train set:
filtered 1148 from  dataset due to extreme length
longest remaining prompt according to tokenizer: 127
longest_test_prompt = 124
Found max n shot per window = 11
Provided labels: dict_values(['world', 'sports', 'business', 'technology'])
Provided labels average n_tokens: 1.0
shortened labels average n_tokens: 1.0
n_tokens_between_shots = 3
shot_lengths = 0          46
1          63
2          60
3          58
4          58
         ... 
119995     67
119996     80
119997     56
119998    116
119999     60
Name: n_tokens, Length: 118852, dtype: int64
prompt_length_percentile = 77.0
max_possible_shots_length = 900
return 11



  0%|          | 0/1 [00:00<?, ?it/s]starting with n = 33
accuracy = 0.0
100%|██████████| 1/1 [00:00<00:00,  1.42it/s]100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
